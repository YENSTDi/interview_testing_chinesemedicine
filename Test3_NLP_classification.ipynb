{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainData(Dataset):\n",
    "  def __init__(self, data, tokenizer) -> None:\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    text = self.data['text'][index]\n",
    "    label = self.data['label'][index]\n",
    "\n",
    "    token = self.tokenizer(text, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    input_ids = token['input_ids']\n",
    "    attention_mask = token['attention_mask']\n",
    "    label = torch.tensor(label)\n",
    "\n",
    "    return (input_ids, attention_mask, label)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(datas):\n",
    "  input_ids = [torch.Tensor(i[0]) for i in datas]\n",
    "  attention_mask = [torch.Tensor(i[1]) for i in datas]\n",
    "  \n",
    "  if datas[0][2] is not None:\n",
    "      labels = torch.stack([i[2] for i in datas])\n",
    "  else:\n",
    "      labels = None\n",
    "\n",
    "  input_ids_tensors = pad_sequence(input_ids, batch_first=True)\n",
    "  masks_tensors = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "  input_ids_tensors   = input_ids_tensors.to(torch.long)\n",
    "  masks_tensors     = masks_tensors.to(torch.long)\n",
    "  \n",
    "  return input_ids_tensors, masks_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data_loader):\n",
    "  loss = 0\n",
    "  model.eval()\n",
    "  prediction = None\n",
    "  true = None\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for index, data in enumerate(data_loader):\n",
    "      input_ids, marks, label= [t.to(\"cuda:0\") for t in data]\n",
    "      \n",
    "      output_loss = model(input_ids, marks, labels=label)\n",
    "      loss += output_loss[0].item()\n",
    "\n",
    "      output = model(input_ids, marks)\n",
    "      logits = output[0]\n",
    "      _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "      pred = pred.cpu()\n",
    "      label = label.cpu()\n",
    "\n",
    "      if prediction is None:\n",
    "        prediction = pred\n",
    "        true = label\n",
    "      else:\n",
    "        prediction = torch.cat((prediction, pred))\n",
    "        true = torch.cat((true, label))\n",
    "\n",
    "  loss = loss / len(data_loader)\n",
    "  acc = accuracy_score(true, prediction)\n",
    "  return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6399, grad_fn=<NllLossBackward0>), logits=tensor([[0.4549, 0.7538],\n",
       "        [0.2158, 0.5265],\n",
       "        [0.0066, 0.4286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count = 2500\n",
    "val_count = 1000\n",
    "\n",
    "torch.random.manual_seed(16)\n",
    "train_index = torch.randint(len(dataset['train']), (train_count,))\n",
    "val_index = torch.randint(len(dataset['test']), (val_count,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MainData(dataset['train'][train_index], tokenizer)\n",
    "val_dataset = MainData(dataset['test'][val_index], tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, collate_fn=create_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=create_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "cls_model = cls_model.to(device)\n",
    "\n",
    "optimizer = AdamW(cls_model.parameters(), lr=4e-6)\n",
    "Train_loss = []\n",
    "Val_loss = []\n",
    "Train_acc = []\n",
    "Val_acc = []\n",
    "\n",
    "EPOCH = 5\n",
    "for epoch in range(EPOCH):\n",
    "  bts = 0\n",
    "  for index, data in enumerate(train_loader):\n",
    "    cls_model.train()\n",
    "    input_ids, marks, label = [t.to(device) for t in data]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = cls_model(input_ids, marks, labels=label)\n",
    "    l = output[0]\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    bts+=1\n",
    "    stats = 'Epoch [%d/%d], Step [%d/%d], Batch-Loss: %.4f' % (epoch+1, EPOCH, bts, len(train_loader), l.item())\n",
    "    print('\\r' + stats, end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "  \n",
    "  train_acc, train_loss = eval(cls_model, train_loader)\n",
    "  val_acc, val_loss = eval(cls_model, val_loader)\n",
    "  Train_loss.append(train_loss)\n",
    "  Val_loss.append(val_loss)\n",
    "  Train_acc.append(train_acc)\n",
    "  Val_acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5), dpi=100, linewidth = 2)\n",
    "plt.plot(Train_loss, 's-', color='r', label=\"Train-Loss\")   \n",
    "plt.plot(Val_loss, 'o-', color='g', label=\"Val-Loss\")    \n",
    "plt.xlabel(\"epoch\", fontsize=15, labelpad = 15)\n",
    "plt.ylabel(\"loss\", fontsize=15, labelpad = 20)\n",
    "plt.legend(loc = \"best\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5), dpi=100, linewidth = 2)\n",
    "plt.plot(Train_acc, 's-', color='r', label=\"Train-Acc\")   \n",
    "plt.plot(val_acc, 'o-', color='g', label=\"Val-Acc\")    \n",
    "plt.xlabel(\"epoch\", fontsize=15, labelpad = 15)\n",
    "plt.ylabel(\"accuracy\", fontsize=15, labelpad = 20)\n",
    "plt.legend(loc = \"best\", fontsize=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "170762d7d3b3a4ea23ce9bbf9f833429f837955d29218776897c7f8c90a19c05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
